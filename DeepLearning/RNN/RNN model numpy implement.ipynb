{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RNN code only with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "1. x and y label\n",
    "2. For word forecast model, 0 stands for SENTENCE_START and 1 stands for SENTENCE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 1:\n",
    "example sentense \\\n",
    "SENTENCE_START what do you think about language processing model, is it good? SENTENCE_END \\\n",
    "full sentence: [0, 51, 27, 16, 10, 856, 53, 25, 34, 69, 12, 13, 43, 41, 1] \\\n",
    "in this example, each number stands for a word, in real problem, normally a vector stands for a word, so the input is a series of vector \n",
    "(also can view as a 2-D array) \\\n",
    "input and output size: [time_steps, input_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([0, 51, 27, 16, 10, 856, 53, 25, 34, 69, 12, 13, 43, 41]).reshape(-1,1)\n",
    "y_train = np.array([51, 27, 16, 10, 856, 53, 25, 34, 69, 12, 13, 43, 41,1]).reshape(-1,1)\n",
    "\n",
    "## we have 6 words: I love go shopping with you, labels as:\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 2:\n",
    "we have 6 words and a sign: **I love going shopping with you.** labels as: \\\n",
    "I    ---------   [1,0,0,0,0,0,0] \\\n",
    "love   ---------  [0,1,0,0,0,0,0] \\\n",
    "going    ---------  [0,0,1,0,0,0,0] \\\n",
    "shopping --------- [0,0,0,1,0,0,0] \\\n",
    "with    --------- [0,0,0,0,1,0,1] \\\n",
    "you   ---------   [0,0,0,0,0,1,0] \\\n",
    ".    ---------   [0,0,0,0,0,0,1] \\\n",
    "and we also need begin and end mark, which is: \\\n",
    "BEGIN --------- [0,0,0,0,0,0,0] \\\n",
    "END --------- [1,1,1,1,1,1,1] \\\n",
    "\n",
    "To training our data, we make a few correct sentences by these words: \\\n",
    "Sentence 1: **I love going shopping with you.** \\\n",
    "Sentence 2: **I love you.**\\\n",
    "Sentence 3: **I love shopping**\\\n",
    "Sentense 4: **I love going with you**\\\n",
    "\n",
    "The Trainng Dataset is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words['I'] = [1,0,0,0,0,0,0]\n",
    "words['love'] = [0,1,0,0,0,0,0]\n",
    "words['going'] = [0,0,1,0,0,0,0]\n",
    "words['shopping'] = [0,0,0,1,0,0,0]\n",
    "words['with'] = [0,0,0,0,1,0,0]\n",
    "words['you'] = [0,0,0,0,0,1,0]\n",
    "words['.'] = [0,0,0,0,0,0,1]\n",
    "words['BEGIN'] = [0,0,0,0,0,0,0]\n",
    "words['END'] = [1,1,1,1,1,1,1]\n",
    "\n",
    "def generate_sentence_data(sentence):\n",
    "    word_list = sentence.split()\n",
    "    word_list.insert(0,'BEGIN')\n",
    "    word_list.append('END')\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0,len(word_list)-1):\n",
    "        current_word = word_list[i]\n",
    "        next_word = word_list[i+1]\n",
    "        x_train.append(words[current_word])\n",
    "        y_train.append(words[next_word])\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_sentence_data('I love going shopping with you .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example 3:\n",
    "# word2vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "1. Init stucture and parameters \n",
    "2. Forward \n",
    "    - Simple RNN\n",
    "    - LSTM\n",
    "3. Backpropogation\n",
    "3. Update parameters\n",
    "4. Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implement is a simple RNN with only one hiden layer\n",
    "\n",
    "\n",
    "remember the forward propagation follow the function: \\\n",
    "\n",
    "$$ s_t =tanh(U^{(sx)}x_t + Ws_{t-1}^{(ss)})$$        \n",
    "$$ \\omicron = softmax(V_{S_t})$$       \n",
    "lost function: $$J^{(t)}(\\theta) = \\sum_{i=1}^{|V|}(y_{t_i}^{'}logy_{t_i})$$   \n",
    "## init function parameter here: \n",
    "size of input data (word dimension): x \\\n",
    "size of previous state (size of output dimension): s \\\n",
    "W(ss),U(sx),V(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the default model is a three layers RNN model with a recurrent hiden layer, and an input, an output layer \\\n",
    "input dimension is **word_dim** \\\n",
    "output dimension is **d** \\\n",
    "number of hiden nodes is **hiden_dim** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    return e_x / e_x.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n",
    "[train](#Train-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim,  hidden_dim))\n",
    "        \n",
    "## input x and output y (a full list through time s--> states   o--> output)\n",
    "## input follow function 1,2\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        if self.word_dim == 1:\n",
    "            T = len(x)\n",
    "        else:\n",
    "            T = x.shape[0]\n",
    "        \n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh((self.U.dot(x[t].reshape(-1,1)) + self.W.dot(s[t-1]).reshape(-1,1))).reshape(self.hidden_dim)\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    " \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "## lost function follow function 3    \n",
    "## It's a cross entropy lost function that calculate classify problem\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in range(0,len(y)):\n",
    "#             print('this is %d th step'%(i))\n",
    "#             print('input training data is',x[:i+1,:].reshape(-1,x.shape[1]))\n",
    "            o, s = self.forward_propagation(x[:i+1,:].reshape(-1,x.shape[1]))\n",
    "#             print('output result is:',o)\n",
    "#             print('status result is: ',s)\n",
    "            # We only care about our prediction of the &quot;correct&quot; words\n",
    "            true_answer = y[i]\n",
    "            predict_answer = o[i]\n",
    "            correct_word_predictions = []\n",
    "            for i in range(0,len(true_answer)):\n",
    "                if true_answer[i] == 1:\n",
    "                    correct_word_predictions.append(predict_answer[i])\n",
    "                else:\n",
    "                    correct_word_predictions.append(1)\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y)) \n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        if self.word_dim == 1:\n",
    "            T = len(x)\n",
    "        else:\n",
    "            T = x.shape[0]\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "#        print('dLdU shape:', self.U.shape, 'dLdV shape:', self.V.shape, 'dLdW shape:', self.W.shape)\n",
    "        ## delta_o is the output error, o is the forwardprop value (0-1)\n",
    "        delta_o = o - y\n",
    "#        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T-1)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "#            print('delta_o shape: ', delta_o.shape, ' delta_t shape', delta_t.shape)\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print &quot;Backpropagation step t=%d bptt step=%d &quot; % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])  \n",
    "#                print('calculate columns: ',x[bptt_step], 'value ', dLdU[:,x[bptt_step]], 'shape: ',dLdU[:,x[bptt_step]].shape)\n",
    "                dLdU[:,bptt_step] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW] \n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print('Performing gradient check for parameter %s with size %d.'%(pname, np.prod(parameter.shape)))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print('Gradient Check ERROR: parameter=%s ix=%s'% (pname, ix))\n",
    "                    print(';+h Loss: %f'% gradplus)\n",
    "                    print(';-h Loss: %f'% gradminus)\n",
    "                    print(';Estimated_gradient: %f' % estimated_gradient)\n",
    "                    print(';Backpropagation gradient: %f'% backprop_gradient)\n",
    "                    print(';Relative Error: %f'% relative_error)\n",
    "                    return it.iternext()\n",
    "            print('Gradient check for parameter %s passed.'% (pname))\n",
    "            \n",
    "    # Performs one step of SGD.\n",
    "    def numpy_sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bptt\n",
    "from the previous function, we know:\\\n",
    "$$ \\frac{\\partial E_3}{\\partial V} = \\frac{\\partial E_3}{\\partial \\hat{y_3}} \\frac{\\partial \\hat{y_3}}{\\partial V} \n",
    "        = (\\hat{y_3}-y_3) * s_3$$\n",
    "$$\\frac{\\partial E_3}{\\partial W} = \\sum^3_{k=0}\n",
    "\\frac{\\partial E_3}{\\partial \\hat{y_3}} \\frac{\\partial \\hat{y_3}}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_k} \\frac{\\partial s_k}{\\partial W}$$\n",
    "To calculate from bptt, we want to get: \\\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_t \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}\n",
    " = \\sum_t \\sum_{k=1}^{t} \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial s_t}\\frac{\\partial s_t}{\\partial s_k}\\frac{\\partial s_k}{\\partial W}$$\n",
    "$$\\frac{\\partial L}{\\partial V} = \\sum_{t=0}^T \\partial O* s_t^T $$ \n",
    "$$\\frac{\\partial L}{\\partial U} = \\sum_t \\frac{\\partial L}{\\partial O}\\frac{\\partial 0}{\\partial s_t}\\frac{\\partial s_t}{\\partial U}$$\n",
    "And update U,V,W by gradient descent:\\\n",
    "$$ U_{n} = U_{n-1} - \\eta * \\frac{\\partial L}{\\partial U_{n-1}}$$\n",
    "$$ V_{n} = V_{n-1} - \\eta * \\frac{\\partial L}{\\partial V_{n-1}}$$\n",
    "$$ W_{n} = W_{n-1} - \\eta * \\frac{\\partial L}{\\partial W_{n-1}}$$\n",
    "\n",
    "This [blog](https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9) and this [paper](https://www.researchgate.net/publication/308980601_A_Gentle_Tutorial_of_Recurrent_Neural_Network_with_Error_Backpropagation) illustrate backpropagation clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathscr{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "verify implement is corrent:\\\n",
    "$$ \\frac{\\partial L}{\\partial \\theta} L \\sim \\lim_{h-> 1} \\frac{J(\\theta + h)- J(\\theta - h)}{2h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "[Model](#RNN-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print ('%s: Loss  after num_examples_seen=%d epoch=%d: %f '% (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print ('Setting learning rate to %f'%learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.numpy_sgd_step(X_train, y_train, learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use example 2 sentence as the training sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init model\n",
    "model = RNN(x_train.shape[1], hidden_dim=10, bptt_truncate=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test each module\n",
    "o,s = model.forward_propagation(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:68: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4806791285061665"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_loss(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[dLdU, dLdV, dLdW] = model.bptt(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-16 20:37:30: Loss  after num_examples_seen=0 epoch=0: 0.480679 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=40 epoch=5: 0.463726 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=80 epoch=10: 0.445329 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=120 epoch=15: 0.423961 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=160 epoch=20: 0.400539 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=200 epoch=25: 0.381503 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=240 epoch=30: 0.368695 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:68: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-16 20:37:30: Loss  after num_examples_seen=280 epoch=35: 0.359040 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=320 epoch=40: 0.352621 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=360 epoch=45: 0.350650 \n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=400 epoch=50: 0.352442 \n",
      "Setting learning rate to 0.002500\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=440 epoch=55: 0.354374 \n",
      "Setting learning rate to 0.001250\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=480 epoch=60: 0.355543 \n",
      "Setting learning rate to 0.000625\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=520 epoch=65: 0.356173 \n",
      "Setting learning rate to 0.000313\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=560 epoch=70: 0.356498 \n",
      "Setting learning rate to 0.000156\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=600 epoch=75: 0.356664 \n",
      "Setting learning rate to 0.000078\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=640 epoch=80: 0.356747 \n",
      "Setting learning rate to 0.000039\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=680 epoch=85: 0.356789 \n",
      "Setting learning rate to 0.000020\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=720 epoch=90: 0.356810 \n",
      "Setting learning rate to 0.000010\n",
      "2020-06-16 20:37:30: Loss  after num_examples_seen=760 epoch=95: 0.356820 \n",
      "Setting learning rate to 0.000005\n"
     ]
    }
   ],
   "source": [
    "train_with_sgd(model, x_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
