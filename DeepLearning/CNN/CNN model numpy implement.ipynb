{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN implement by numpy\n",
    "\n",
    "Reference blog: \n",
    "- [Building Convolutional Neural Network using NumPy from Scratch](https://towardsdatascience.com/building-convolutional-neural-network-using-numpy-from-scratch-b30aac50e50a) \n",
    "- [Convolutional Neural Networks from the ground up](https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as ps\n",
    "import sklearn \n",
    "import skimage\n",
    "import tensorflow.keras.datasets as datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "- use MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = skimage.data.chelsea()\n",
    "img = skimage.color.rgb2gray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skimage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a6db0934e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'skimage' is not defined"
     ]
    }
   ],
   "source": [
    "skimage.io.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-74512cd07d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mnist.npz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test)=datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-30602697b5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "- Build a basic CNN model with an **input layer**, a **cnn layer**, a **downsample layer** and a **dense layer** model\n",
    "- use zero-padding\n",
    "\n",
    "- filter in shape (number_of_filter, number_of_color, filter_x, filter_y)\n",
    "- image in shape **[number_of_color, image_x, image_y]** (remember in Keras model, training data(image) formed in shape **[number_of_images,image_x, image_y, number_of_color]**)\n",
    "\n",
    "- loss function for cross entropy:\n",
    "\n",
    "$$ H(y,y^{hat}) = \\sum_i y_i log \\frac{1}{y_i^{hat}} = - \\sum_i y_i log y_i^{hat}  $$\n",
    "\n",
    "- back propagation \\\n",
    "blogs: \\\n",
    " [Convolutions and Backpropagations](https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c) \\\n",
    "we can also apply chain role to convolution layer \\\n",
    "\n",
    "for **convolution layer**, we have filter as `F` and input data(image) is `x`, Conv(X,F) we can get the output `O` and final Loss `L`: \n",
    "\n",
    "$$ \\frac{\\alpha L}{\\alpha F} = Conv (x, \\frac{\\alpha L}{\\alpha O})$$\n",
    "$$ \\frac{\\alpha L}{\\alpha x} = Conv (180^o rotated F, \\frac{\\alpha L}{\\alpha O})$$\n",
    "\n",
    "We need to update parameters `W`,`b` and `kernel_filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn():\n",
    "\n",
    "    ## input size is a tuple=(n_color, input_x, input_y)\n",
    "    def __init__(self,input_size,kernel_size,n_filters,output_size=1,stride=1):\n",
    "        ## parameters in input layer\n",
    "        self.input_size = input_size\n",
    "        self.n_c = input_size[0]\n",
    "        self.img_x = input_size[1]\n",
    "        self.img_y = input_size[2]\n",
    "        \n",
    "        ## parameters in cnn layer\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_filters = n_filters\n",
    "        self.filter = np.random.uniform(size=(self.n_filters,self.n_c,self.kernel_size,self.kernel_size))\n",
    "        self.bias = np.random.uniform(size=(self.n_filters))\n",
    "        self.output_size = output_size\n",
    "        self.stride = stride\n",
    "\n",
    "        \n",
    "        ## parameters in dense layer\n",
    "        out_dim_x = int((input_size[1] - kernel_size+2)/stride) + 1\n",
    "        out_dim_y = int((input_size[2] - kernel_size+2)/stride) + 1\n",
    "        n_flatten_size = n_filters*input_size[0]*(int(out_dim_x/2) +int(out_dim_x%2))*(int(out_dim_y/2) +int(out_dim_y%2))\n",
    "        self.W = np.random.uniform(-1,1,size = (n_flatten_size,output_size))\n",
    "        self.b = np.random.uniform(size = (output_size,1))\n",
    "        \n",
    "    def forward_propagation(self,x):\n",
    "        cnn_out = Relu(self.convolution(x,self.filter,stride=self.stride))\n",
    "        max_pool_out = self.maxpool(cnn_out)\n",
    "        flatten_out = self.flatten(max_pool_out)\n",
    "        output = self.W.T.dot(flatten_out)+self.b\n",
    "    \n",
    "        return softmax(output)\n",
    "    \n",
    "    def back_propagation(self,y):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward_backward_propagation(self,x,y):\n",
    "        cnn_out = Relu(self.convolution(x,self.filter,stride=self.stride))\n",
    "        max_pool_out = self.maxpool(cnn_out)\n",
    "        flatten_out = self.flatten(max_pool_out)\n",
    "        output = self.W.T.dot(flatten_out)+self.b\n",
    "        y_out = softmax(output)\n",
    "        \n",
    "        loss = self.calculate_total_loss(y,y_out)\n",
    "        \n",
    "        dout = y_out - np.array(y).reshape(self.output_size,-1)\n",
    "\n",
    "        dW = dout.dot(flatten_out.T)\n",
    "        db = np.sum(dout,axis=1).reshape(self.b.shape)\n",
    "        \n",
    "        dflatten_out = self.W.dot(dout)\n",
    "        dpool = dflatten_out.reshape(max_pool_out.shape)\n",
    "        \n",
    "        dconv = maxpoolBackward(dpool,cnn_out,2,2)\n",
    "        dconv[cnn_out<0]=0\n",
    "        \n",
    "        dimage,dfilter,dbias = convolutionBackward(dconv,x,self.filter,self.stride)\n",
    "        \n",
    "        grads = [dfilter,dbias, dW, db]\n",
    "        \n",
    "        return grads,loss\n",
    "    \n",
    "    \n",
    "    ## adamGD gradient algorithm   \n",
    "    ## train one batch of data, update parameters\n",
    "    # x_batch in shape (number_of_data, n_color, dim_x, dim_y) , y_batch is the label of data\n",
    "    # parameters for AdamGD: lr, beta1, beta2\n",
    "    def adamGD(self, x_batch, y_batch, lr, beta1, beta2, cost):\n",
    "  \n",
    "        num_classes = self.output_size\n",
    "\n",
    "        X = x_batch # get batch inputs\n",
    "        Y = y_batch # get batch labels\n",
    "\n",
    "        cost_ = 0\n",
    "        batch_size = len(x_batch)\n",
    "\n",
    "        # initialize gradients and momentum,RMS params\n",
    "        dfilter = np.zeros(self.filter.shape)\n",
    "#        df2 = np.zeros(f2.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "#        dw4 = np.zeros(w4.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "#        db2 = np.zeros(b2.shape)\n",
    "        dbias = np.zeros(self.bias.shape)\n",
    "#        db4 = np.zeros(b4.shape)\n",
    "\n",
    "        vfilter = np.zeros(self.filter.shape)\n",
    "#        v2 = np.zeros(f2.shape)\n",
    "        vW = np.zeros(self.W.shape)\n",
    "#        v4 = np.zeros(w4.shape)\n",
    "        vbias = np.zeros(self.bias.shape)\n",
    "#        bv2 = np.zeros(b2.shape)\n",
    "        vb = np.zeros(self.b.shape)\n",
    "#        bv4 = np.zeros(b4.shape)\n",
    "\n",
    "        sfilter = np.zeros(self.filter.shape)\n",
    "#        s2 = np.zeros(f2.shape)\n",
    "        sW = np.zeros(self.W.shape)\n",
    "#        s4 = np.zeros(w4.shape)\n",
    "        sbias = np.zeros(self.bias.shape)\n",
    "#        bs2 = np.zeros(b2.shape)\n",
    "        sb = np.zeros(self.b.shape)\n",
    "#        bs4 = np.zeros(b4.shape)\n",
    "\n",
    "        for i in range(0,batch_size):\n",
    "\n",
    "            x = X[i]\n",
    "            y = np.eye(num_classes)[int(Y[i])].reshape(num_classes, 1) # convert label to one-hot\n",
    "\n",
    "            # Collect Gradients for training example\n",
    "            grads, loss = self.forward_backward_propagation(x, y)\n",
    "            [dfilter_,dbias_, dW_, db_] = grads\n",
    "\n",
    "            dfilter+=dfilter_\n",
    "            dbias+=dbias_.reshape(-1)\n",
    "            dW+=dW_.T\n",
    "            db+=db_\n",
    "\n",
    "            cost_+= loss\n",
    "\n",
    "        # Parameter Update  \n",
    "\n",
    "        vfilter = beta1*vfilter + (1-beta1)*dfilter/batch_size # momentum update\n",
    "        sfilter = beta2*sfilter + (1-beta2)*(dfilter/batch_size)**2 # RMSProp update\n",
    "        self.filter -= lr * vfilter/np.sqrt(sfilter+1e-7) # combine momentum and RMSProp to perform update with Adam\n",
    "\n",
    "        vbias = beta1*vbias + (1-beta1)*dbias/batch_size\n",
    "        sbias = beta2*sbias + (1-beta2)*(dbias/batch_size)**2\n",
    "        self.bias -= lr * vbias/np.sqrt(sbias+1e-7)\n",
    "\n",
    "  \n",
    "        vW = beta1*vW + (1-beta1) * dW/batch_size\n",
    "        sW = beta2*sW + (1-beta2)*(dW/batch_size)**2\n",
    "        self.W -= lr * vW/np.sqrt(sW+1e-7)\n",
    "\n",
    "        vb = beta1*vb + (1-beta1) * db/batch_size\n",
    "        sb = beta2*sb + (1-beta2)*(db/batch_size)**2\n",
    "        self.b -= lr * vb/np.sqrt(sb+1e-7)\n",
    "\n",
    "        cost_ = cost_/batch_size\n",
    "        cost.append(cost_)\n",
    "\n",
    "        #params = [self.filter,self.bias, self.W, self.b]\n",
    "\n",
    "        return  cost\n",
    "\n",
    "    #####################################################\n",
    "    ##################### Training ######################\n",
    "    #####################################################\n",
    "\n",
    "    def train(self, x , y, lr=0.005, beta1 = 0.95, beta2 = 0.99, batch_size = 32, num_epochs = 2, save_path = 'params.pkl'):\n",
    "\n",
    "        # Get training data\n",
    "#        x,y = shuffle(x,y, random_state = 41)\n",
    "        m =50000\n",
    "\n",
    "        ## Initializing all the parameters\n",
    "        num_class = self.output_size\n",
    "\n",
    "        cost = []\n",
    "\n",
    "        print(\"LR:\"+str(lr)+\", Batch Size:\"+str(batch_size))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            x,y = shuffle(x,y)\n",
    "            x_batches = [x[k:k + batch_size] for k in range(0, x.shape[0], batch_size)]\n",
    "            y_batches = [y[k:k + batch_size] for k in range(0, y.shape[0], batch_size)]\n",
    "    \n",
    "            for i in range(0,len(x_batches)):\n",
    "                cost = []\n",
    "                x_batch = x_batches[i]\n",
    "                y_batch = y_batches[i]\n",
    "                cost = self.adamGD(x_batch, y_batch,  lr, beta1, beta2 , cost)\n",
    "                print(\"Cost: %.2f\" % (cost[-1]))\n",
    "            \n",
    "            print('final cost in this epoch: ',np.mean(cost))\n",
    "\n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as file:\n",
    "                pickle.dump(params, file)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def predict(self,x):\n",
    "        output = []\n",
    "        for i in range(0,len(x)):\n",
    "            y_onehot= self.forward_propagation(x[i].reshape(self.n_c,self.img_x,self.img_y))\n",
    "            output.append(y_onehot)\n",
    "        return np.argmax(output,axis=1)\n",
    "        \n",
    "\n",
    "    ## cross entropy\n",
    "    def calculate_total_loss(self,y,y_hat):\n",
    "        return -np.sum(y*np.log(y_hat+1e-15))\n",
    "        \n",
    "\n",
    "    ## do convolution for a single image and output the result \n",
    "    def convolution(self,image,kernel_filter,stride=1):\n",
    "        \n",
    "        ## claculate the input and output dimension and make sure they match\n",
    "        (n_f,n_c_f,f_x,f_y ) = kernel_filter.shape\n",
    "        (n_c,img_x,img_y) = image.shape\n",
    "        \n",
    "        out_dim_x = int((img_x - f_x+2)/stride) + 1\n",
    "        out_dim_y = int((img_y - f_y+2)/stride) + 1       \n",
    "        \n",
    "        assert n_c == n_c_f\n",
    "        \n",
    "        ## zero-padding\n",
    "        Image = np.zeros((img_x+2,img_y+2))\n",
    "        Image[1:img_x+1,1:img_y+1] = image \n",
    "        \n",
    "        ## calculate output\n",
    "        out = np.zeros((n_f,out_dim_x,out_dim_y))\n",
    "        \n",
    "        for i,curr_f in enumerate(kernel_filter):\n",
    "            curr_out = np.zeros((out_dim_x,out_dim_y))\n",
    "            for out_x in range(0,out_dim_x):\n",
    "                for out_y in range(0,out_dim_y):\n",
    "                    image_here = Image[out_x*stride:out_x*stride+f_x, out_y*stride:out_y*stride+f_y]\n",
    "                    curr_out[out_x,out_y] = np.sum(curr_f * image_here)\n",
    "                    \n",
    "            out[i] = curr_out + self.bias[i]\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    ## strik image to 1/4 by combine each four pix to the max one\n",
    "    def maxpool(self,image):\n",
    "        (n_c,img_x,img_y) = image.shape\n",
    "        out = np.zeros((n_c, int(img_x/2)+ int(img_x%2),int(img_y/2)+ int(img_y%2)))\n",
    "        for current_c in range(0,n_c):\n",
    "            for i in range(0,out.shape[1]):\n",
    "                for j in range(0,out.shape[2]):\n",
    "                    if (i+1 < out.shape[1]) and (j+1 < out.shape[2]):\n",
    "                        out[current_c,i,j] = np.max(image[current_c,2*i:2*i+2,2*j:2*j+2])\n",
    "                    elif (i+1 == out.shape[1]) and (j+1) == out.shape[2]:\n",
    "                        out[current_c,i,j] = image[current_c,2*i,2*j]\n",
    "                    elif (i+1 < out.shape[1]) and (j+1) == out.shape[2]:\n",
    "                        out[current_c,i,j] = np.max(image[current_c,2*i:2*i+2,2*j])\n",
    "                    elif (i+1 == out.shape[1]) and (j+1) < out.shape[2]:\n",
    "                        out[current_c,i,j] = np.max(image[current_c,2*i,2*j:2*j+2])                    \n",
    "                  \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def flatten(self,x):\n",
    "        return x.reshape(-1,1)\n",
    "    \n",
    "    \n",
    "\n",
    "def softmax(raw_preds):\n",
    "    \n",
    "    preds = raw_preds-np.max(raw_preds)\n",
    "    out = np.exp(preds) # exponentiate vector of raw predictions\n",
    "    return out/np.sum(out) # divide the exponentiated vector by its sum. All values in the output sum to 1.\n",
    "    \n",
    "    \n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "        \n",
    "    \n",
    "## in the backpropagation of cov layer, \n",
    "## we use filter(f/filt) and input data(x/conv_in), difference of output(dconv_prev --> could be an output of  packprop from the frontier layer \n",
    "## as the input of deconv calculation\n",
    "## we use difference of filter(dfilt), difference of the bias(dbias), difference of the out(dout) as the output\n",
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    (n_f, n_c, f, _) = filt.shape\n",
    "    (_, orig_dim_x, orig_dim_y) = conv_in.shape\n",
    "    ## initialize derivatives\n",
    "    dout = np.zeros(conv_in.shape) \n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((n_f,1))\n",
    "    for curr_f in range(n_f):\n",
    "        # loop through all filters\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim_y:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim_x:\n",
    "                # loss gradient of filter (used to update the filter)\n",
    "                dfilt[curr_f] += dconv_prev[curr_f, out_x, out_y] * conv_in[:, curr_x:curr_x+f, curr_y:curr_y+f]\n",
    "                # loss gradient of the input to the convolution operation (conv1 in the case of this network)\n",
    "                dout[:, curr_x:curr_x+f, curr_y:curr_y+f] += dconv_prev[curr_f, out_x, out_y] * filt[curr_f] \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        # loss gradient of the bias\n",
    "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
    "    \n",
    "    return dout, dfilt, dbias\n",
    "\n",
    "def nanargmax(arr):\n",
    "\n",
    "##    return index of the largest non-nan value in the array. Output is an ordered pair tuple\n",
    "\n",
    "    idx = np.nanargmax(arr)\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "\n",
    "## in the backpropagation of maxpool layer\n",
    "##\n",
    "def maxpoolBackward(dpool, orig, f, s):\n",
    "##    Backpropagation through a maxpooling layer. The gradients are passed through the indices of greatest value in the original maxpooling during the forward step.\n",
    "\n",
    "    (n_c, orig_dim_x, orig_dim_y) = orig.shape\n",
    "    \n",
    "    dout = np.zeros(orig.shape)\n",
    "    \n",
    "    for curr_c in range(n_c):\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim_y:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim_x:\n",
    "                # obtain index of largest value in input for current window\n",
    "                (a, b) = nanargmax(orig[curr_c, curr_x:curr_x+f, curr_y:curr_y+f])\n",
    "                dout[curr_c, curr_x+a, curr_y+b] = dpool[curr_c, out_x, out_y]\n",
    "                \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        \n",
    "    return dout        \n",
    "\n",
    "def DenseBackward():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d100aa0ce9f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-94f719dcfbe3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, kernel_size, n_filters, output_size, stride)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_c\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "model= cnn(input_size=(1,28,28),kernel_size=3,n_filters=10,output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train(x_train.reshape(-1,1,28,28) , np.array(y_train), beta1 = 0.95, beta2 = 0.99, batch_size = 32, num_epochs = 2, save_path = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4dc05ec8f797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(x_train[3:20].reshape(-1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
